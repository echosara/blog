---
layout:     post
title:      "神经网络学习笔记"
subtitle:   "第一弹"
date:       2017-08-23
author:     "Echo"
header-img: "img/post-bg-NN.jpg"
catalog: true
tags:
    - Machine Learning
---



 > 向万能的大计算机低头

# 神经网络的一些东西

## Softmax函数

softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的k维向量z的“压缩”到另一个k维实向量$\sigma(z)$中，使得每一个元素的范围都在（0,1）之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：

$$\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum^{K}_{k=1}e^{z_{k}}}$$for j=1,....,K

softmax函数实际上是有限项离散概率分布的梯度对数归一化。因此，softmax函数在包括多项逻辑回归，多项线性判别分析，朴素贝叶斯分类器和人工神经网络等基于概率的多分类问题方法中都有广泛应用。这个函数通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

#### 超参数

深度神经网络模型对应有两套参数，一类我们称之为基础参数（elementary parameter），如卷积层或全连接层的权重（weight）和偏置项（bias），另一类就是超参数（hyperparameter），如网络训练时的学习率，损失函数中L2正则化项的系数。在实际应用中，深度神经网络要取得好的性能，非常依赖于选择出一组好的超参数。超参数的意思就是有未知参数的参数。

#### BP神经网络

BP神经网络是一种多层前馈神经网络，主要特点信号向前传播误差向后传播。误差后向传播，

假设$X_{1},X_{2},\cdots,X_{n}$是BP神经网络的输入值，$Y_{1},Y_{2},\cdots,Y_{m}$是BP神经网络的预测值，$\omega_{ij},\omega_{jk}$为BP神经网络的权值。BP神经网络可以看成是一个非线性函数，当输入节点数为n，输出节点数为m时，BP神经网络就表达了从n个自变量到m个因变量的函数映射关系。

##### BP训练步骤

1. 网络初始化。根据系统输入输出序列（X，Y）确定网络输入层节点数n，隐含层节点数l，输出层节点数m，初始化输入层、隐含层和输出层神经元之间的连接权值$\omega_{ij},\omega_{jk}$,初始化隐含层阈值a，输出层阈值b，给定hpyerparamenter学习速率和神经元激励函数。

2. 隐含层输出计算。根据输入向量X，输入层和隐含层间的连接权值$\omega_{ij}$以及隐含层阈值a，计算隐含层的输出H

​                             $H=f(\sum^{n}_{i=1}\omega_{ij}x_{i}-a_{j}$                   j=1,2,....l

3. 输出层输出计算。根据隐含层输出H,连接权值$\omega_{ij}$和阈值b，计算BP神经网络预测输出O。
  ​          $O_{k}=\sum^{i}_{j=1}H_{j}\omega_{jk}-b_{k}$           k=1,2...m
4. 误差计算。根据网络预测输出O和期望输出Y，计算网络预测误差e.                        
  $e_{k}=Y_{k}-O_{k}$
5. 权值更新。根据网络的预测误差e更新网络连接权值$\omega_{ij},\omega_{jk}$
  非输出节点的权值$\omega_{ij} = \omega_{ij}+\eta H_{j}(1-H_{j})x(i)\sum^{m}_{k=1}\omega_{jk}e_{k}$ i=1,2,...n;j=1,2,...l
  输出节点的权值$\omega_{jk}=\omega_{jk}+\eta H_{j}e_{k}$      j=1,2...m
  式中$\eta$为学习效率（learning rate），也就是按梯度降低的step size，中间用了多元微分求导
6. 阈值更新。根据网络预测误差e更新网络节点阈值a,b.
  $a_{j}=a_{j}+\eta H_{j}(1-H_{j})\sum^{m}_{k=1}\omega_{jk}e_{k}$ j=1,2,...l
  $b_{k}= b_{k}+e_{k}$   k=1,2,...m
7. 判断算法迭代是否结束，若没有结束，返回步骤2

##### 梯度计算

在计算梯度（gradient）的时候有两种方法，数值法和解析法。数值法用一个很小的趋近于0 的数计算梯度，但是对于计算机来说成本很高，计算机能很容易算出梯度的公式，但是在实现过程中会有很多错误，所以实际上，普遍的做法是用解析法进行计算，用数值法进行检验。称为梯度检验（gradient check）

下面用ＳＶＭ损失函数(loss function)作为例子对于 a single datapoint

$L_{i}=\sum_{j\neq y_{i}}[max(0,w_{j}^{T}x_{i}-w^{T}_{y_{i}}x_{i}+\Delta)]$

$\frac{dL_{i}}{dw_{y_{i}}}=-(\sum_{j\neq y_{i}}1(w_{j}^{T}x_{i}-w^{T}_{y_{i}}x_{i}+\Delta)>0)x_{i}$

这里的1表示括号能的条件为真式子就为1，为假就为0，

对其他列的W求导

$\frac{L_{i}}{w_{i}}=1(w^{T}_{j}x_{i}-w^{T}_{y_{i}}x_{i}+\Delta)x_{i}$

除了梯度下降还有其他方法进行优化（LBFGS）



神经网络对于曲线的拟合和线性函数对曲线的拟合其实是一样的，都有一个error function，也都是调整参数使得error function变小

# 激活函数

## sigmoid 函数

任何一个可微的函数都可以作为一个激活函数，激活函数表示神经突触之间激活的频率。

$f(w,x)=\frac{1}{1+e^{-(w_{0}x_{0}+w_{1}x_{1}+w_{2})}}$

这个函数计算微分的时候有一个技巧

$\sigma(x)=\frac{1}{1+e^{-x}}$

$\frac{d\sigma(x)}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1-\sigma(x))\sigma(x)$

 它可以把实数域上的数据压缩到0-1之间，0表示没有激活，1表示达到最大频率。现在这个函数很少使用，主要因为下面两个缺陷：

1. sigmoid函数在极正或者极负的时候会饱合，然后使得梯度很小，然后网络就很难进行学习。
2. sigmoid产生的函数都是正数，这样对权重求梯度时，会呈现一个z字形，相比于上面那个问题这个情况问题不是很大。

## Tanh函数

Tanh函数会把实数域压缩到[-1,1]之间，和sigmoid函数一样，它也会饱合，但是是以0为对称中心的。其实，tanh函数可以用sigmoid函数表示

$$\tanh(x)=2\sigma(2x)-1$$

## ReLU函数

这个函数最近几年很流行，f(x)=max(0,x)

### Pros

1. 相比于非线性的函数，在随机梯度下降时收敛速度大大加快，这被认为是因为它是线性的，没有非线性饱合现象
2. 和sigmoid和Tanh函数复杂的操作，比如指数操作相比，ReLU函数的操作比较简单。

### cons

1. 不幸的是，ReLU函数单元，在训练过程中会比较脆弱，可能死亡。但是具体的形式自己还是不知道

## Leaky ReLU（泄露的ReLU函数）

在x<0的时候，让这个函数有一个很小的负的斜率，可以用下面的公式表示

$f(x)=\mathbb{1}(x<0)(\alpha x)+\mathbb{1}(x>=0)(x)$， 

$\alpha$是一个非常小的函数这个函数是不是一定有用还不清楚。

## Maxout

----

所以应该选择什么样的神经元类型呢？使用非线性的ReLU函数，要去注意学习速率，监督死亡的现象。如果这难倒你了，可以试下Leaky ReLU函数和Maxout函数，不要去用sigmoid函数，也不要指望Tanh函数会出什么好结果。

## 径向基函数神经网络(RBF，Radical Basis Function)

具有单隐层的前馈网络。由于它模拟了人脑中的局部调整、相互覆盖接受域（或称感受域，Receptive Field）的神经网络，因此RBF网络是一种局部逼近的网络，能以任意精度逼近任一连续函数。

1. RBF网和BP网的主要的不同点是在非线性映射上采用了不同的作用函数，分别是径向基函数和Sigmoid函数，前者作用函数是局部的，如高斯RBF，后者作用时全局的
2. RBF网络隐节点的中心Ci是个困难的问题。
3. 径向基函数，即径向对称函数有多种，最常用的是高斯RBF。
4. 高斯RBF网络用于非线性系统辨识与控制，虽具有维一最佳逼近的特性以及无局部极小的优点，但是隐节点的中心和标准化参数难求，这是该网络难以广泛使用的原因。

## PID神经网络

单隐层前馈网络，具有非线性，其隐层节点分别是比例、积分、微分单元，因此是动态网络

## recursive（递归） 与 iterative（迭代）的区别

所谓递归，简而言之就是应用程序自身调用自身，以实现层次 数据结构 的查询和访问。递归的使用可以使代码更简洁清晰，可读性更好（但是对于新手倒不见得），但是递归需要系统堆栈，所以空间消耗要比非递归代码要大很多，而且，如果递归深度太大，可能系统资源会不够用。

往往有这样的观点：能不用递归就不用递归，递归都可以用迭代来代替。

诚然，在理论上，递归和迭代在时间的复杂度方面是等价的（在不考虑函数调用开销和函数调用所产生的堆栈开销）

函数开销和堆栈开销是迭代在运行过程中，程序会用堆栈的数据结构去存储临时变量的值，然后会占用内存。这是我的理解，当然可能会有错。

递归和迭代都是循环的一种。

简单来说，递归（recursive）是重复调用函数自身实现循环。迭代是函数内某段代码实现循环。而迭代与普通循环的区别是：循环代码中参与运算的变量同时是保存结果的变量，当前保存的结果作为下一次循环的初始值

递归循环中，遇到满足终止条件的情况时逐层返回来结束。迭代则使用计数器结束循环。当然很多情况都是多种循环混合采用。

递归的例子，比如给定一个整数数组，采用折半查询返回指定值在数组中的索引，假设数组已经排序。

迭代经典的例子就是实数的累加 

*Naming conventions*   **我们在计算神经网络层数的时候不算输入层**,因此，一层的神经网络就是没有隐含层的，输入层直接联系输出层，在这个层面上，你常常会听到人们把logistics回归和支持向量机称为是一层神经网络的特殊形式。

一般用神经元的数量和参数的数量去形容神经网络的大小

有一个隐含层的神经网络，当然，非线性函数要选择妥当，可以逼近任意连续函数，可见[*Approximation by Superpositions of Sigmoidal Function*](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf) from 1989 (pdf),or this [intuitive explanation](http://neuralnetworksanddeeplearning.com/chap4.html) from Michael Nielsen

**这一层感觉想把原来的数据分布提升一个维度，然后就变得可分了，可以从XOR函数中得到启发**

那如果一层隐含层就够了，那为什么还要更多的层，要更深入呢？回答是，两层神经网络也是可以逼近任意函数，并且在数学上更加可爱。有多个隐含层的神经网络比单层的工作更加有效是通过经验主义的结论，实际上他们拟合函数的能力是一样的。另外经常是这种情况，3层神经网络要比两层的表现更加出色，但是更加深入时，很少会有更加好的效果。但是卷积神经网络的层数就很重要，一种说法是，图像是一种分层结构，脸由耳朵构成，耳朵又是由边缘组成。当然这是一个相当新的话题，深入了解的话可以看下面的书

[Deep Learning](http://www.deeplearningbook.org/) book in press by Bengio, Goodfellow, Courville, in particular [Chapter 6.4](http://www.deeplearningbook.org/contents/mlp.html).

[Do Deep Nets Really Need to be Deep?](http://arxiv.org/abs/1312.6184)

[FitNets: Hints for Thin Deep Nets](http://arxiv.org/abs/1412.6550)

所以我们应该怎么选择神经网络的规模呢？神经元数量的增大可以使模型表现更加复杂的函数，但是随之而来的坏处是，过拟合。那对于少量的数据我们也不只能用小型的神经网络来训练以 防止过拟合，还有其他防止过拟合的方式。



# echoecho你的头又大了

###### L-MBP是什么？

###### Maxout函数

好像返回的参数要多一点

> put some bells and whistles on the loop 是什么意思

> ILSVRC challenge 又是什么

> the art ConvNets又好像很有趣的样子

###### 支持向量机

k均值聚类怎么调整聚类中心

###### 小脑模型神经网络（CMAC）

前馈网络，有两个基本映射 概念映射（U->AC） 实际映射（AC->AP），表示输入/输出的非线性关系。但是这两个映射我都不懂，

概念映射是从输入空间U至概念（虚拟）存储器AC的映射。

实际映射（Practical Mapping） 由概念存储器AC的c个单元用杂散编码（Hash Coding）技术映射至实际存储器AP的c个单元。c个单元中存放相应的权重，则网络的输出为AP中c个单元的权值的和。第一步不懂第二步就别想懂了。

#### 一些术语

SGD-> Stochastic Gradient Descent 随机梯度下降

MGD->Minibatch Gradient Descent 小批量梯度下降	 

BGD-> Batch Gradient Descent         批量梯度下降

cross-entropy loss 